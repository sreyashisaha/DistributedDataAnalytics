{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ead1e0f",
   "metadata": {},
   "source": [
    "# Sreyashi Saha\n",
    "# Matriculation Number : 1747271"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f315d559",
   "metadata": {},
   "source": [
    "## Exercise 1 Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc5f5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f64d1222",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "list_b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfbb2c7",
   "metadata": {},
   "source": [
    "Create two RDD objects of a, b and do the following tasks. Words should be remained in the results of join operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ccea591",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/03 10:33:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "22/07/03 10:33:30 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n"
     ]
    }
   ],
   "source": [
    "sc =SparkContext()\n",
    "a_rdd = sc.parallelize(list_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c183d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_rdd = sc.parallelize(list_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bed89642",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['spark', 'rdd', 'python', 'context', 'create', 'class']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14e7ad1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['operation', 'apache', 'scala', 'lambda', 'parallel', 'partition']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb70b5a",
   "metadata": {},
   "source": [
    "In order to perform the joins we need to have key value pairs. Therefore, we map each word from both the list and assign their values as 1 using the map function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50fd929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_a = a_rdd.map(lambda x: (x, 1)).collect()\n",
    "map_a = a_rdd.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58baa6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map_b = b_rdd.map(lambda x: (x, 1)).collect()\n",
    "map_b = b_rdd.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a089d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark', 1),\n",
       " ('rdd', 1),\n",
       " ('python', 1),\n",
       " ('context', 1),\n",
       " ('create', 1),\n",
       " ('class', 1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_a.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44241e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('operation', 1),\n",
       " ('apache', 1),\n",
       " ('scala', 1),\n",
       " ('lambda', 1),\n",
       " ('parallel', 1),\n",
       " ('partition', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_b.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4596925",
   "metadata": {},
   "source": [
    "We can see that all the words in both rdds now have values assigned as 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e41c9",
   "metadata": {},
   "source": [
    "\n",
    "1. Perform rightOuterJoin and fullOuterJoin operations between a and b. Briefly explain your solution. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d1ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "join1 = map_a.rightOuterJoin(map_b)\n",
    "Right_Outer_Join = join1.map(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4dec8eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['parallel', 'lambda', 'scala', 'operation', 'apache', 'partition']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Right_Outer_Join.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8dba7a",
   "metadata": {},
   "source": [
    "A right outer join is a method of combining tables. The result includes unmatched rows from only the table that is specified after the RIGHT OUTER JOIN phrase. Here in our case we have map_b after the phrase, it returns all records from the right rdd (map_b), and the matching records from the left rdd (map_a). The result is 0 records from the left side, if there is no match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9234c523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "join2 = map_a.fullOuterJoin(map_b)\n",
    "Full_Outer_join = join2.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "585bf915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['python',\n",
       " 'spark',\n",
       " 'context',\n",
       " 'create',\n",
       " 'parallel',\n",
       " 'lambda',\n",
       " 'class',\n",
       " 'rdd',\n",
       " 'scala',\n",
       " 'operation',\n",
       " 'apache',\n",
       " 'partition']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Full_Outer_join.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5bd77f",
   "metadata": {},
   "source": [
    "The FULL OUTER JOIN keyword returns all records when there is a match in left rdd (map_a) or right rdd(map_b) table records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f76caf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 1),\n",
       " ('spark', 1),\n",
       " ('context', 1),\n",
       " ('create', 1),\n",
       " ('parallel', 1),\n",
       " ('lambda', 1),\n",
       " ('class', 1),\n",
       " ('rdd', 1),\n",
       " ('scala', 1),\n",
       " ('operation', 1),\n",
       " ('apache', 1),\n",
       " ('partition', 1)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_list = sc.parallelize(Full_Outer_join)\n",
    "all_words = new_list.map(lambda word: (word, 1)).collect()\n",
    "all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa229f",
   "metadata": {},
   "source": [
    "Here I map all the words after performing Full_Outer_join and take them as key value pairs for further implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308ebac1",
   "metadata": {},
   "source": [
    "2. Using map and reduce functions to count how many times the character \"s\" appears in all a and b. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb910280",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = new_list.map(lambda x: (x, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef38fc08",
   "metadata": {},
   "source": [
    "In the function below I check if the word received as input has the character 's' in it or not. If it does then I count the number of times I get 's' for that word and then I return the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33e747fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_s(temp):\n",
    "    c=0\n",
    "    for idx in temp:\n",
    "        if idx == 's':\n",
    "                c+=1\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e33f419",
   "metadata": {},
   "source": [
    "Here I map the words and the number of 's' that the word contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeedf9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_s_words = words.map(lambda x:(x[0], check_for_s(x[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86d506a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('python', 0),\n",
       " ('spark', 1),\n",
       " ('context', 0),\n",
       " ('create', 0),\n",
       " ('parallel', 0),\n",
       " ('lambda', 0),\n",
       " ('class', 2),\n",
       " ('rdd', 0),\n",
       " ('scala', 1),\n",
       " ('operation', 0),\n",
       " ('apache', 0),\n",
       " ('partition', 0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_s_words.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0106cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = count_s_words.map(lambda a:a[1]).reduce(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8195363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The number of times s occurs in the list of words using reduce funtion is  4\n"
     ]
    }
   ],
   "source": [
    "print(\" The number of times s occurs in the list of words using reduce funtion is \",count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23daf0",
   "metadata": {},
   "source": [
    "3. Using aggregate function to count how many times the character \"s\" appears in all a and b. (1 point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4516c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "count1 = words.aggregate(0,lambda a,x:a+check_for_s(x[0]), lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ae3a42f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The number of times s occurs in the list of words using aggregate funtion is  4\n"
     ]
    }
   ],
   "source": [
    "print(\" The number of times s occurs in the list of words using aggregate funtion is \",count1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb21cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
